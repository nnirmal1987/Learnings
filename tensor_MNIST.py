# -*- coding: utf-8 -*-
"""tensor_py3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fUsX64ykNoVkesNWnd9FDVa2llSphoPE
"""

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

!pip install pydrive

mnist=input_data.read_data_sets("MNIST_data/",one_hot=True)
type(mnist)

mnist.validation.num_examples

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt

# %matplotlib inline

plt.imshow(mnist.train.images[1].reshape(28,28),cmap=plt.get_cmap('Blues'))

x=tf.placeholder(tf.float32,shape=[None,784])
w=tf.Variable(tf.zeros([784,10]))
b=tf.Variable(tf.zeros([10]))


y=tf.matmul(x,w)+b

tf.variables_initializer

y_true=tf.placeholder(tf.float32,shape=[None,10])

cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,logits=y))

optimizer=tf.train.GradientDescentOptimizer(0.4).minimize(cross_entropy)



init=tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
     
    for step in range(1200):
      
        batch_x, batch_y = mnist.train.next_batch(100)
        
        sess.run(train,feed_dict={x:batch_x,y_true:batch_y})
     
    matches=tf.equal(tf.argmax(y,1),tf.argmax(y_true,1))
    acc=tf.reduce_mean(tf.cast(matches,tf.float32))
    print(sess.run(acc,feed_dict={x:mnist.test.images,y_true:mnist.test.labels}))

def sigmoid(x):
  return (1/1+np.exp(-x))

n_input=3
n_hidden=2
n_output=1

weight={'hidden':tf.Variable(tf.ones([n_input,n_hidden])),
        'out':tf.Variable(tf.ones([n_hidden,n_output]))
}

bias={'bias':tf.Variable(tf.ones([n_hidden])),
        'bout':tf.Variable(tf.ones([n_output]))
}

x=tf.placeholder(float,[none,n_input])
y=tf.placeholder(float,[none,n_output])
l1=tf.add(tf.matmull(x,weight['hidden'],bias['bias']))
l1_actual=tf.sigmoid(l1)

out=tf.add(tf.matmull(l1_actual,weight['out'],bias['bout']))
out_actual=tf.sigmoid(out)

cost=tf.reduce_mean(tf.abs(tf.subtract(out_actual,y)))

train=tf.train.AdadeltaOptimizer(learning_rate=0.2).minimize(cost)

